import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns


!wget -O datasets/tutorial3.csv "https://docs.google.com/spreadsheets/d/1lS9HEz8FreAkCpR_w_ykAHy3oWeQBqiuc_RJyr3dfbE/export?format=csv&gid=1322780754"


file_path = 'datasets/tutorial3.csv'
df = pd.read_csv(file_path)


df





# Correlation
class DataFrame:
    def __init__(self, file_path):
        self.df = pd.read_csv(file_path)

    def calculate_correlation_matrix(self):
        # Get numerical columns
        df = self.df
        numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
        num_cols = len(numerical_columns)

        # Create an empty correlation matrix
        corr_matrix = {}
        for row in numerical_columns:
            corr_matrix[row] = {}
            for col in numerical_columns:
                corr_matrix[row][col] = 0

        for i in range(num_cols):
            for j in range(i, num_cols):
                col1 = numerical_columns[i]
                col2 = numerical_columns[j]

                # Calculate mean
                mean1 = df[col1].mean()
                mean2 = df[col2].mean()

                # Calculate covariance
                covariance = ((df[col1] - mean1) * (df[col2] - mean2)).mean()

                # Calculate standard deviations
                std_dev1 = df[col1].std()
                std_dev2 = df[col2].std()

                # Calculate correlation
                correlation = covariance / (std_dev1 * std_dev2)

                # Store in matrix
                corr_matrix[col1][col2] = correlation
                corr_matrix[col2][col1] = correlation  # Symmetric matrix

        return corr_matrix

    def display_correlation_matrix(self):
        corr_matrix = self.calculate_correlation_matrix()
        print("Correlation Matrix:")
        for col1, row in corr_matrix.items():
            print(f"{col1}: {row}")
        # print(len(corr_matrix))
        # print(len(corr_matrix[0]))
        data_frame = pd.DataFrame(corr_matrix) 
        plt.figure(figsize=(10,8))
        sns.heatmap(data_frame, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, center=0)
        plt.title('Correlation Matrix Heatmap')
        plt.show()


        

file_path = 'datasets/tutorial3.csv'
df = DataFrame(file_path)
df.display_correlation_matrix()


# Mean, median and mode
file_path = 'datasets/tutorial3.csv'
df = pd.read_csv(file_path)
numeric_df = df.select_dtypes(include=['float64', 'int64'])
# numeric_df = df.iloc[: -2]

for column in numeric_df.columns:
    # Retrieve the column data as a list
    data = numeric_df[column].dropna().tolist()  # Use dropna() to exclude NA values

    # Calculate mean
    total_sum = sum(data)
    count = len(data)
    mean = total_sum / count

    # Calculate median
    sorted_data = sorted(data)
    mid = count // 2
    if count % 2 == 0:
        median = (sorted_data[mid - 1] + sorted_data[mid]) / 2
    else:
        median = sorted_data[mid]

    # Calculate mode
    frequency = {}
    for value in data:
        if value in frequency:
            frequency[value] += 1
        else:
            frequency[value] = 1
    mode = max(frequency, key=frequency.get)

    # Print the results
    print(f"\nColumn: {column}")
    print(f"Mean: {mean}")
    print(f"Median: {median}")
    print(f"Mode: {mode}")










# Filling in missing values
def count_na_values(df):
    na_counts = {col: 0 for col in df.columns}

    # Iterate through each row and column
    for index, row in df.iterrows():
        for col in df.columns:
            # Check for NA values
            if pd.isna(row[col]):
                na_counts[col] += 1

    print("Columns with NA values:")
    for column, count in na_counts.items():
        # if count > 0:
        print(f"{column}: {count}")
        # else: 
            # print("No columns with NA values")

# numerical_columns = df.select_dtypes(include=['float64', 'int64']) 
count_na_values(df)

df_copy = df.copy()
df_copy.drop(['Museum ID', 'Museum Name','Legal Name','Alternate Name','Institution Name','Zip Code (Physical Location)','Phone Number','Employer ID Number', 'Street Address (Physical Location)', 'City (Physical Location)', 'State (Physical Location)', 'Street Address (Administrative Location)'], axis=1, inplace=True)
# Custom function to fill missing values with the mean
pd.options.mode.chained_assignment = None  # default='warn'

def fill_missing_with_mean(data, numerical_columns):
    for column in numerical_columns:
        total_sum = 0
        count = 0
        for value in data[column]:
            if not pd.isna(value):  # Check for missing values
                total_sum += value
                count += 1
        mean_value = total_sum / count

        # Replace missing values with the calculated mean
        for i in range(len(data[column])):
            if pd.isna(data[column][i]) or data[column][i] == 0:
                data[column][i] = mean_value

numerical_columns = df_copy.select_dtypes(include=['float64', 'int64']).columns
fill_missing_with_mean(df_copy, numerical_columns)
count_na_values(df_copy)
df_copy



# Encoding
# Hint: Figure out which columns need to be encoded. Would One-Hot Encoding be enough for handling this type of data? What are the alternatives?
df_copy2 = df_copy.copy()
df_copy2.drop(['City (Administrative Location)', 'State (Administrative Location)'], axis=1, inplace=True)
categorical_columns = df_copy2.select_dtypes(include=['object', 'category']).columns

unique_counts = df_copy2[categorical_columns].nunique()

plt.figure(figsize=(10, 6))
unique_counts.sort_values().plot(kind='barh', color='skyblue')
plt.xlabel('Number of Unique Values')
plt.ylabel('Categorical Features')
plt.title('Number of Unique Values in Categorical Features')
plt.show()

plt.figure(figsize=(18, 18))
plt.title('Categorical Features: Distribution')
plt.xticks(rotation=90)
index = 1

for col in categorical_columns:
    y = df[col].value_counts()
    plt.subplot(11, 4, index)
    plt.xticks(rotation=90)
    sns.barplot(x=list(y.index), y=y)
    index += 1

df_copy2


count_na_values(df_copy)



def one_hot_encode(df, columns):
  """
  Args:
    df: The pandas DataFrame containing the data.
    column_name: The name of the column to one-hot encode.

  Returns:
    The DataFrame with the one-hot encoded columns.
  """
  for column_name in columns:
      unique_values = df[column_name].unique()
      for value in unique_values:
        df[column_name + "(" + str(value) + ")"] = (df[column_name] == value).astype(int)
      df = df.drop(column_name, axis=1)
  return df

df_encoded = one_hot_encode(df_copy2.copy(), ['Museum Type'])
print(df_encoded.head())


df_encoded



