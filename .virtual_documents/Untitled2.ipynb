import math
import random
import csv

def mean(data):
    return sum(data) / len(data)

def std_dev(data):
    avg = mean(data)
    return math.sqrt(sum((x - avg) ** 2 for x in data) / len(data))

def dot_product(a, b):
    return sum(x * y for x, y in zip(a, b))

def matrix_multiply(A, B):
    rows_A = len(A)
    cols_B = len(B[0])
    cols_A = len(A[0])
    
    # Initialize
    result = [[0] * cols_B for _ in range(rows_A)]
    
    # Perform matrix multiplication
    for i in range(rows_A):
        for j in range(cols_B):
            for k in range(cols_A):
                result[i][j] += A[i][k] * B[k][j]
    
    return result

def transpose(matrix):
    return list(map(list, zip(*matrix)))

def subtract_vectors(a, b):
    return [x - y for x, y in zip(a, b)]

def add_vectors(a, b):
    return [x + y for x, y in zip(a, b)]

def scalar_multiply(scalar, vector):
    return [scalar * x for x in vector]

def element_wise_multiply(a, b):
    return [x * y for x, y in zip(a, b)]

# Helper functions for matrix operations

def minor(matrix, row, col):
    return [r[:col] + r[col + 1:] for i, r in enumerate(matrix) if i != row]

def cofactor(matrix):
    n = len(matrix)
    cof = []
    for row in range(n):
        cof_row = []
        for col in range(n):
            minor_det = matrix_determinant(minor(matrix, row, col))
            cof_row.append(((-1) ** (row + col)) * minor_det)
        cof.append(cof_row)
    return cof

def matrix_inverse(matrix):
    det = determinant(matrix)
    if det == 0:
        raise ValueError("Matrix is singular and cannot be inverted.")
    
    cof_matrix = cofactor(matrix)
    adjugate = transpose(cof_matrix)
    
    # Divide each element in the adjugate by the determinant
    inverse = [[adjugate[i][j] / det for j in range(len(adjugate))] for i in range(len(adjugate))]
    
    return inverse

def matrix_determinant(matrix):
    n = len(matrix)
    if n == 1:
        return matrix[0][0]
    if n == 2:
        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]
    det = 0
    for j in range(n):
        submatrix = [row[:j] + row[j+1:] for row in matrix[1:]]
        det += (-1)**j * matrix[0][j] * matrix_determinant(submatrix)
    return det
    
def normalize_data(data):
    min_vals = [min(col) for col in zip(*data)]
    max_vals = [max(col) for col in zip(*data)]
    return [[(x - min_val) / (max_val - min_val) for x, min_val, max_val in zip(row, min_vals, max_vals)] for row in data]

# Implement PCA
def pca(data, n_components):
    # Center the data
    mean_vector = [mean(col) for col in zip(*data)]
    centered_data = [subtract_vectors(row, mean_vector) for row in data]

    # Compute covariance matrix
    cov_matrix = [[sum(a * b for a, b in zip(col1, col2)) / (len(data) - 1)
                   for col2 in zip(*centered_data)] for col1 in zip(*centered_data)]

    # Compute eigenvectors and eigenvalues
    eigenvalues, eigenvectors = eigen_decomposition(cov_matrix)

    # Sort eigenvectors by eigenvalues in descending order
    sorted_indices = sorted(range(len(eigenvalues)), key=lambda k: eigenvalues[k], reverse=True)
    top_eigenvectors = [eigenvectors[i] for i in sorted_indices[:n_components]]

    # Project data onto principal components
    return [matrix_multiply([row], transpose(top_eigenvectors))[0] for row in centered_data]

# Simple implementation of eigendecomposition (power iteration method)
def eigen_decomposition(matrix, num_iterations=100):
    n = len(matrix)
    eigenvalues = []
    eigenvectors = []

    for _ in range(n):
        vector = [random.random() for _ in range(n)]
        for _ in range(num_iterations):
            new_vector = matrix_multiply(matrix, [[x] for x in vector])
            new_vector = [x[0] for x in new_vector]
            norm = math.sqrt(sum(x*x for x in new_vector))
            vector = [x / norm for x in new_vector]
        
        eigenvalue = dot_product(matrix_multiply(matrix, [[x] for x in vector])[0], vector)
        eigenvalues.append(eigenvalue)
        eigenvectors.append(vector)

        # Deflate the matrix
        outer_product = [[a * b for b in vector] for a in vector]
        matrix = [[matrix[i][j] - eigenvalue * outer_product[i][j] for j in range(n)] for i in range(n)]

    return eigenvalues, eigenvectors

# Gaussian Mixture Model implementation
class GaussianMixture:
    def __init__(self, n_components, n_features, max_iter=100, tol=1e-3):
        self.n_components = n_components
        self.n_features = n_features
        self.max_iter = max_iter
        self.tol = tol
    
    def initialize_parameters(self, X):
        n_samples = len(X)
        self.weights = [1.0 / self.n_components] * self.n_components
        self.means = random.sample(X, self.n_components)
        # Add a small value to the diagonal of the covariance matrices
        self.covariances = [[[1.0 if i == j else 0.0 for j in range(self.n_features)] for i in range(self.n_features)] for _ in range(self.n_components)]
        for cov in self.covariances:
            for i in range(self.n_features):
                cov[i][i] += 1e-6

    def gaussian_pdf(self, x, mean, cov):
        n = len(x)
        diff = subtract_vectors(x, mean)
        try:
            inv_cov = matrix_inverse(cov)
            det_cov = matrix_determinant(cov)
            if det_cov == 0:
                return 0.0
            exponent = -0.5 * dot_product(matrix_multiply([diff], inv_cov)[0], diff)
            return math.exp(exponent) / math.sqrt((2 * math.pi)**n * det_cov)
        except ZeroDivisionError:
            return 0.0
    
    def expectation_step(self, X):
        responsibilities = [[0.0 for _ in range(self.n_components)] for _ in range(len(X))]
        for i, x in enumerate(X):
            total = 0.0
            for k in range(self.n_components):
                responsibilities[i][k] = self.weights[k] * self.gaussian_pdf(x, self.means[k], self.covariances[k])
                total += responsibilities[i][k]
            responsibilities[i] = [r / total for r in responsibilities[i]]
        return responsibilities
    
    def maximization_step(self, X, responsibilities):
        n_samples = len(X)
        total_resp = [sum(r[k] for r in responsibilities) for k in range(self.n_components)]
        
        self.weights = [total / n_samples for total in total_resp]
        
        for k in range(self.n_components):
            self.means[k] = [sum(r[k] * x[j] for r, x in zip(responsibilities, X)) / total_resp[k] for j in range(self.n_features)]
            
            diff = [subtract_vectors(x, self.means[k]) for x in X]
            weighted_diff = [scalar_multiply(r[k], d) for r, d in zip(responsibilities, diff)]
            self.covariances[k] = [[sum(wd[i] * d[j] for wd, d in zip(weighted_diff, diff)) / total_resp[k] for j in range(self.n_features)] for i in range(self.n_features)]
    
    def fit(self, X):
        self.initialize_parameters(X)
        
        for _ in range(self.max_iter):
            prev_log_likelihood = self.log_likelihood(X)
            responsibilities = self.expectation_step(X)
            self.maximization_step(X, responsibilities)
            curr_log_likelihood = self.log_likelihood(X)
            
            if abs(curr_log_likelihood - prev_log_likelihood) < self.tol:
                break
    
    def log_likelihood(self, X):
        return sum(math.log(sum(self.weights[k] * self.gaussian_pdf(x, self.means[k], self.covariances[k]) for k in range(self.n_components))) for x in X)
    
    def predict(self, X):
        responsibilities = self.expectation_step(X)
        return [max(range(self.n_components), key=lambda k: r[k]) for r in responsibilities]



matrix = [[1,2,3],[4,5,6],[7,8,9]]
for i, r in enumerate(matrix):
    if(i!=1):
        print(r[:1]+r[2:])



