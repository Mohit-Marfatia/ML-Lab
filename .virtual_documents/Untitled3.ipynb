import pandas as pd

# Load the dataset
file_path = 'datasets/moonDataset.csv'
data = pd.read_csv(file_path)

# Show the first few rows of the dataset to understand its structure
data.head()


dataList = data.values.tolist()


def calculate_mean(column):
    return sum(column) / len(column)

def calculate_std_dev(column, mean):
    variance = sum((x - mean) ** 2 for x in column) / (len(column) - 1)
    return variance ** 0.5

def calculate_covariance(col1, col2, mean1, mean2):
    return sum((col1[i] - mean1) * (col2[i] - mean2) for i in range(len(col1))) / (len(col1) - 1)

columns = list(zip(*dataList))
num_cols = len(columns)
column_names = ['X1', 'X2', 'X3', 'label']

corr_matrix = {}

# Initialize corr_matrix with column names as keys
for col1 in column_names:
    corr_matrix[col1] = {}

# Loop over all pairs of columns and calculate the correlation
for i in range(num_cols):
    for j in range(i, num_cols):
        col1 = columns[i]
        col2 = columns[j]

        # Calculate means
        mean1 = calculate_mean(col1)
        mean2 = calculate_mean(col2)

        # Calculate covariance
        covariance = calculate_covariance(col1, col2, mean1, mean2)

        # Calculate standard deviations
        std_dev1 = calculate_std_dev(col1, mean1)
        std_dev2 = calculate_std_dev(col2, mean2)

        # Calculate correlation
        if std_dev1 != 0 and std_dev2 != 0:  # Avoid division by zero
            correlation = covariance / (std_dev1 * std_dev2)
        else:
            correlation = 0  # If any standard deviation is zero, correlation is zero

        # Store the correlation in the symmetric matrix
        corr_matrix[column_names[i]][column_names[j]] = correlation
        corr_matrix[column_names[j]][column_names[i]] = correlation  # Symmetry

# Print the correlation matrix
print("Correlation Matrix:")
for col1, row in corr_matrix.items():
    print(f"{col1}: {row}")
    
data_frame = pd.DataFrame(corr_matrix) 
plt.figure(figsize=(10,8))
sns.heatmap(data_frame, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, center=0)
plt.title('Correlation Matrix Heatmap')
plt.show()


def calculate_percentile(column, percentile):
    sorted_col = sorted(column)
    index = (len(sorted_col) - 1) * percentile
    lower = int(index)
    upper = lower + 1
    weight = index - lower
    if upper < len(sorted_col):
        return sorted_col[lower] * (1 - weight) + sorted_col[upper] * weight
    else:
        return sorted_col[lower]

# Function to describe the data
def describe_data(columns, column_names):
    description = {}
    for i, col in enumerate(columns):
        col_data = list(col)
        mean = calculate_mean(col_data)
        std = calculate_std_dev(col_data, mean)
        description[column_names[i]] = {
            'count': len(col_data),
            'mean': mean,
            'std': std,
            'min': min(col_data),
            '25%': calculate_percentile(col_data, 0.25),
            '50%': calculate_percentile(col_data, 0.50),
            '75%': calculate_percentile(col_data, 0.75),
            'max': max(col_data)
        }
     # Print the table header (including Max)
    header = "{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}".format(
        "Column", "Count", "Mean", "Std", "Min", "25%", "50%", "75%", "Max"
    )
    print(header)
    print("-" * len(header))

    # Print the rows with the stats (including Max)
    for col, stats in description.items():
        row = "{:<10} {:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}".format(
            col, stats['count'], stats['mean'], stats['std'], stats['min'],
            stats['25%'], stats['50%'], stats['75%'], stats['max']
        )
        print(row)

# Get description of the data
describe_data(columns, column_names)



import seaborn as sns
sns.set_style("darkgrid")
colors = sns.color_palette(palette = 'dark')

fig,ax = plt.subplots(nrows = 1, ncols = 3, figsize = (12, 3.4))
ax = ax.flat

for i,col in enumerate(data.columns[:-1]):
    sns.histplot(data, x = col, stat = 'density', color = colors[i], fill = False, ax = ax[i])
    sns.kdeplot(data, x = col, color = colors[i], fill = True, ax = ax[i])
    sns.rugplot(data, x = col, color = colors[i], ax = ax[i])
    ax[i].set_xlabel("")
    ax[i].set_title(f"{col}", fontweight = 'bold')
    
fig.tight_layout()
fig.show()


X1 = [row[0] for row in dataList]
X2 = [row[1] for row in dataList]
X3 = [row[2] for row in dataList]
labels = [row[3] for row in dataList]

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))
fig.suptitle('Scatter Plots of X1, X2, and X3', fontsize=16)

# Function to create scatter plot
def create_scatter(ax, x, y, xlabel, ylabel, title):
    scatter = ax.scatter(x, y, c=labels, cmap='viridis', s=50, edgecolors='k', alpha=0.7)
    ax.set_xlabel(xlabel, fontsize=12)
    ax.set_ylabel(ylabel, fontsize=12)
    ax.set_title(title, fontsize=14)
    ax.grid(True, linestyle='--', alpha=0.7)
    return scatter

# X1 vs X2
scatter1 = create_scatter(ax1, X1, X2, 'X1', 'X2', 'X1 vs X2')

# X2 vs X3
scatter2 = create_scatter(ax2, X2, X3, 'X2', 'X3', 'X2 vs X3')

# X1 vs X3
scatter3 = create_scatter(ax3, X1, X3, 'X1', 'X3', 'X1 vs X3')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for overall title
plt.show()


import plotly.express as px

labels_str = ['Class 0' if label == 0 else 'Class 1' for label in labels]

fig = px.scatter_3d(
    x=X1,
    y=X2,
    z=X3,
    color=labels_str,  # Color the points based on the label
    labels={'x': 'X1', 'y': 'X2', 'z': 'X3'},  # Axis labels
    title='3D Scatter Plot of the Moon Dataset'
)

fig.update_layout(
    width=1000,  # Set width (in pixels)
    height=700   # Set height (in pixels)
)
# Show the plot
fig.show()


import math
import random

def mean(data):
    return sum(data) / len(data)

def std_dev(data):
    avg = mean(data)
    return math.sqrt(sum((x - avg) ** 2 for x in data) / len(data))

def dot_product(a, b):
    return sum(x * y for x, y in zip(a, b))

def matrix_multiply(A, B):
    rows_A = len(A)
    cols_B = len(B[0])
    cols_A = len(A[0])
    
    # Initialize
    result = [[0] * cols_B for _ in range(rows_A)]
    
    # Perform matrix multiplication
    for i in range(rows_A):
        for j in range(cols_B):
            for k in range(cols_A):
                result[i][j] += A[i][k] * B[k][j]
    
    return result

def transpose(matrix):
    return list(map(list, zip(*matrix)))

def subtract_vectors(a, b):
    return [x - y for x, y in zip(a, b)]

def add_vectors(a, b):
    return [x + y for x, y in zip(a, b)]

def scalar_multiply(scalar, vector):
    return [scalar * x for x in vector]

def element_wise_multiply(a, b):
    return [x * y for x, y in zip(a, b)]

# Helper functions for matrix operations

def minor(matrix, row, col):
    return [r[:col] + r[col + 1:] for i, r in enumerate(matrix) if i != row]

def cofactor(matrix):
    n = len(matrix)
    cof = []
    for row in range(n):
        cof_row = []
        for col in range(n):
            minor_det = matrix_determinant(minor(matrix, row, col))
            cof_row.append(((-1) ** (row + col)) * minor_det)
        cof.append(cof_row)
    return cof

def matrix_inverse(matrix):
    det = matrix_determinant(matrix)
    if det == 0:
        raise ValueError("Matrix is singular and cannot be inverted.")
    
    cof_matrix = cofactor(matrix)
    adjugate = transpose(cof_matrix)
    
    # Divide each element in the adjugate by the determinant
    inverse = [[adjugate[i][j] / det for j in range(len(adjugate))] for i in range(len(adjugate))]
    
    return inverse

def matrix_determinant(matrix):
    n = len(matrix)
    if n == 1:
        return matrix[0][0]
    if n == 2:
        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]
    det = 0
    for j in range(n):
        submatrix = [row[:j] + row[j+1:] for row in matrix[1:]]
        det += (-1)**j * matrix[0][j] * matrix_determinant(submatrix)
    return det
    
def normalize_data(data):
    min_vals = [min(col) for col in zip(*data)]
    max_vals = [max(col) for col in zip(*data)]
    return [[(x - min_val) / (max_val - min_val) for x, min_val, max_val in zip(row, min_vals, max_vals)] for row in data]

# Simple implementation of eigendecomposition (power iteration method)
def eigen_decomposition(matrix, num_iterations=100):
    n = len(matrix)
    eigenvalues = []
    eigenvectors = []

    for _ in range(n):
        vector = [random.random() for _ in range(n)]
        for _ in range(num_iterations):
            new_vector = matrix_multiply(matrix, [[x] for x in vector])
            new_vector = [x[0] for x in new_vector]
            norm = math.sqrt(sum(x*x for x in new_vector))
            vector = [x / norm for x in new_vector]
        
        eigenvalue = dot_product(matrix_multiply(matrix, [[x] for x in vector])[0], vector)
        eigenvalues.append(eigenvalue)
        eigenvectors.append(vector)

        # Deflate the matrix
        outer_product = [[a * b for b in vector] for a in vector]
        matrix = [[matrix[i][j] - eigenvalue * outer_product[i][j] for j in range(n)] for i in range(n)]

    return eigenvalues, eigenvectors


# Helper function: Matrix A^T A
def ata(matrix):
    return matrix_multiply(transpose(matrix), matrix)

# SVD computation using A^T A
def svd_ata(X, k=2):
    # Step 1: Compute A^T A
    AtA = ata(X)

    # Step 2: Eigen decomposition of A^T A to get eigenvalues and eigenvectors
    eigenvalues, eigenvectors = eigen_decomposition(AtA)

    # Step 3: Sort eigenvalues and corresponding eigenvectors
    eigen_pairs = sorted(zip(eigenvalues, eigenvectors), reverse=True, key=lambda pair: pair[0])

    sorted_eigenvalues = [pair[0] for pair in eigen_pairs]
    sorted_eigenvectors = [pair[1] for pair in eigen_pairs]

    # Step 4: Compute singular values as the square roots of eigenvalues
    singular_values = [math.sqrt(ev) for ev in sorted_eigenvalues]

    # Step 5: V is the matrix of eigenvectors of A^T A (corresponds to V.T)
    V = sorted_eigenvectors

    # Step 6: Compute U using the formula U = A * V * Σ^(-1)
    # Σ^(-1) is the inverse of the diagonal matrix of singular values
    S_inv = [[1 / singular_values[i] if i == j else 0 for j in range(len(singular_values))] for i in range(len(singular_values))]
    U = matrix_multiply(matrix_multiply(X, V), S_inv)

    # Step 7: Reduce the dimension
    U_k = [row[:k] for row in U]  # Top-k U
    Vt_k = [row[:k] for row in transpose(V)]  # Top-k V.T
    s_k = singular_values[:k]  # Top-k singular values

    return U_k, s_k, Vt_k

def center_data(matrix):
    n = len(matrix)
    m = len(matrix[0])
    
    # Subtract the mean of each column from the data
    centered_matrix = [[matrix[i][j] - mean([row[j] for row in matrix]) for j in range(m)] for i in range(n)]
    
    return centered_matrix

def pca_svd(X, k=2):
    # Step 1: Mean center the data
    X_centered = center_data(X)

    # Step 2: Perform SVD on the centered data
    U, s, Vt = svd_ata(X_centered, k=k)

    Vt = transpose(Vt)  # Ensure Vt has correct dimensions (2 x 3)
    
    # Step 3: Construct diagonal matrix S from singular values
    S = [[s[i] if i == j else 0 for j in range(k)] for i in range(k)]  # Diagonal matrix for top-k singular values
    
    # Debugging the dimensions
    print(f"U dimensions: {len(U)} x {len(U[0])}")
    print(f"S dimensions: {len(S)} x {len(S[0])}")
    print(f"Vt dimensions: {len(Vt)} x {len(Vt[0])}")
    
    # Step 4: Compute X_hat (reconstructed matrix)
    X_hat = matrix_multiply(matrix_multiply(U, S), Vt)
    
    # Debugging the dimensions of X_hat
    print(f"X_hat dimensions: {len(X_hat)} x {len(X_hat[0])}")
    
    # Step 5: Directly use U to reduce the dimensionality of X
    X_reduced = matrix_multiply(U, S)  # Reduced data using top k singular values

    return X_reduced



X = data[['X1', 'X2', 'X3']].values
y = data['label'].values

X_pca = pca_svd(X)


X_pca_1 = [row[0] for row in X_pca]  # First principal component
X_pca_2 = [row[1] for row in X_pca]  # Second principal component

# Scatter plot with different colors for each label
plt.figure(figsize=(10, 6))
plt.scatter(X_pca_1, X_pca_2, c=y, cmap='viridis', s=50, edgecolors='k', alpha=0.7)

# Adding labels and title
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('2D PCA of Moon Dataset')

# Display color bar for the labels (if applicable)
plt.colorbar(label='Label')

# Show plot
plt.show()


# Split data into training and testing sets
train_ratio = 0.8
split_index = int(len(X_pca) * train_ratio)
X_train, X_test = X_pca[:split_index], X_pca[split_index:]
y_train, y_test = y[:split_index], y[split_index:]


class GaussianMixture:
    def __init__(self, n_components, n_features, max_iter=100, tol=1e-3):
        self.n_components = n_components
        self.n_features = n_features
        self.max_iter = max_iter
        self.tol = tol

    def initialize_parameters(self, X):
        self.weights = [1.0 / self.n_components] * self.n_components
        self.means = random.sample(X, self.n_components)  # Randomly select initial means
        self.covariances = [
            [[1.0 if i == j else 0.0 for j in range(self.n_features)] for i in range(self.n_features)]
            for _ in range(self.n_components)
        ]

    def gaussian_pdf(self, x, mean, cov):
        n = len(x)
        diff = subtract_vectors(x, mean)
        inv_cov = matrix_inverse(cov)
        
        # Calculate the exponent part
        # Properly format the multiplication
        diff_transpose = [[val] for val in diff]  # Convert to column vector
        exponent = -0.5 * dot_product(diff, matrix_multiply(inv_cov, diff_transpose)[0])
        
        det_cov = matrix_determinant(cov)
        return math.exp(exponent) / math.sqrt((2 * math.pi) ** n * det_cov)

    def expectation_step(self, X):
        weighted_likelihood = [[0.0] * self.n_components for _ in range(len(X))]
        for k in range(self.n_components):
            for i in range(len(X)):
                weighted_likelihood[i][k] = self.weights[k] * self.gaussian_pdf(X[i], self.means[k], self.covariances[k])
        
        # Normalize responsibilities
        responsibilities = []
        for i in range(len(X)):
            total_likelihood = sum(weighted_likelihood[i])
            responsibilities.append([likelihood / total_likelihood for likelihood in weighted_likelihood[i]])
        
        return responsibilities

    def maximization_step(self, X, responsibilities):
        total_resp = [0.0] * self.n_components
        for i in range(len(X)):
            for k in range(self.n_components):
                total_resp[k] += responsibilities[i][k]

        self.weights = [resp / len(X) for resp in total_resp]

        # Update means
        self.means = [[0.0] * self.n_features for _ in range(self.n_components)]
        for k in range(self.n_components):
            for i in range(len(X)):
                for j in range(self.n_features):
                    self.means[k][j] += responsibilities[i][k] * X[i][j]
            for j in range(self.n_features):
                self.means[k][j] /= total_resp[k]

        # Update covariances
        for k in range(self.n_components):
            diff = [[X[i][j] - self.means[k][j] for j in range(self.n_features)] for i in range(len(X))]
            self.covariances[k] = [[0.0] * self.n_features for _ in range(self.n_features)]
            for i in range(len(X)):
                for j in range(self.n_features):
                    for l in range(self.n_features):
                        self.covariances[k][j][l] += responsibilities[i][k] * diff[i][j] * diff[i][l]
            for j in range(self.n_features):
                for l in range(self.n_features):
                    self.covariances[k][j][l] /= total_resp[k]

    def fit(self, X):
        self.initialize_parameters(X)

        for _ in range(self.max_iter):
            prev_log_likelihood = self.log_likelihood(X)
            responsibilities = self.expectation_step(X)
            self.maximization_step(X, responsibilities)
            curr_log_likelihood = self.log_likelihood(X)

            if abs(curr_log_likelihood - prev_log_likelihood) < self.tol:
                break

    def log_likelihood(self, X):
        total_log_likelihood = 0.0
        for i in range(len(X)):
            likelihoods = [
                self.weights[k] * self.gaussian_pdf(X[i], self.means[k], self.covariances[k])
                for k in range(self.n_components)
            ]
            total_log_likelihood += math.log(sum(likelihoods))
        return total_log_likelihood

# Fit Gaussian Mixture Models for each class
gmm_class_0 = GaussianMixture(n_components=2, n_features=len(X_train[0]))
gmm_class_1 = GaussianMixture(n_components=2, n_features=len(X_train[0]))

X_train_0 = [X_train[i] for i in range(len(X_train)) if y_train[i] == 0]
X_train_1 = [X_train[i] for i in range(len(X_train)) if y_train[i] == 1]

gmm_class_0.fit(X_train_0)
gmm_class_1.fit(X_train_1)

# Predict using GMMs for each class
def predict_gmm(X):
    log_likelihood_0 = [gmm_class_0.log_likelihood([x]) for x in X]
    log_likelihood_1 = [gmm_class_1.log_likelihood([x]) for x in X]
    return [1 if ll1 > ll0 else 0 for ll0, ll1 in zip(log_likelihood_0, log_likelihood_1)]

# Predict on test set
y_pred = predict_gmm(X_test)

# Evaluate the model
correct_predictions = sum([1 for i in range(len(y_pred)) if y_pred[i] == y_test[i]])
accuracy = correct_predictions / len(y_test)

# Calculate confusion matrix manually
conf_matrix = [[0, 0], [0, 0]]  # Confusion matrix format: [[TN, FP], [FN, TP]]
for true, pred in zip(y_test, y_pred):
    if true == 0 and pred == 0:
        conf_matrix[0][0] += 1  # True Negative
    elif true == 0 and pred == 1:
        conf_matrix[0][1] += 1  # False Positive
    elif true == 1 and pred == 0:
        conf_matrix[1][0] += 1  # False Negative
    elif true == 1 and pred == 1:
        conf_matrix[1][1] += 1  # True Positive

# Output results
print(f'Accuracy: {accuracy * 100:.2f}%')
print('Confusion Matrix:')
print(conf_matrix)


# Assuming you already have the reduced dataset X_pca and labels y
X_test_1 = [row[0] for row in X_test]  # First principal component
X_test_2 = [row[1] for row in X_test]  # Second principal component

# Scatter plot with different colors for each label
plt.figure(figsize=(10, 6))
plt.scatter(X_test_1, X_test_2, c=y_test, cmap='viridis', s=50, edgecolors='k', alpha=0.7)

# Adding labels and title
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('y_actual')

# Display color bar for the labels (if applicable)
plt.colorbar(label='Label')

# Show plot
plt.show()


# Assuming you already have the reduced dataset X_pca and labels y
X_test_1 = [row[0] for row in X_test]  # First principal component
X_test_2 = [row[1] for row in X_test]  # Second principal component

# Scatter plot with different colors for each label
plt.figure(figsize=(10, 6))
plt.scatter(X_test_1, X_test_2, c=y_pred, cmap='viridis', s=50, edgecolors='k', alpha=0.7)

# Adding labels and title
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('y_pred')

# Display color bar for the labels (if applicable)
plt.colorbar(label='Label')

# Show plot
plt.show()



