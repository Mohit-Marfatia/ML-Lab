import pandas as pd

# Load the dataset
file_path = 'datasets/moonDataset.csv'
moon_dataset = pd.read_csv(file_path)

# Show the first few rows of the dataset to understand its structure
moon_dataset.head()


dataList = moon_dataset.values.tolist()


dataList


for i in range(len(dataList)):
    if dataList[i][-1] == 0:
        dataList[i][-1] = -1.0

# dataList


corrMatrix = {'X1': {'X1': 0, 'X2': 0, 'X3': 0, 'label': 0},
 'X2': {'X1': 0, 'X2': 0, 'X3': 0, 'label': 0},
 'X3': {'X1': 0, 'X2': 0, 'X3': 0, 'label': 0},
 'label': {'X1': 0, 'X2': 0, 'X3': 0, 'label': 0}}

corrMatrix


import matplotlib.pyplot as plt
import seaborn as sns

columns = list(zip(*dataList))

# Calculate the number of columns
num_cols = len(columns)

# Initialize the correlation matrix
corr_matrix = {}

# Function to calculate mean
def calculate_mean(column):
    return sum(column) / len(column)

# Function to calculate standard deviation
def calculate_std_dev(column, mean):
    variance = sum((x - mean) ** 2 for x in column) / (len(column) - 1)
    return variance ** 0.5

# Function to calculate covariance
def calculate_covariance(col1, col2, mean1, mean2):
    return sum((col1[i] - mean1) * (col2[i] - mean2) for i in range(len(col1))) / (len(col1) - 1)

# Column names (assuming the structure from your moon dataset)
column_names = ['X1', 'X2', 'X3', 'label']

# Initialize corr_matrix with column names as keys
for col1 in column_names:
    corr_matrix[col1] = {}

# Loop over all pairs of columns and calculate the correlation
for i in range(num_cols):
    for j in range(i, num_cols):
        col1 = columns[i]
        col2 = columns[j]

        # Calculate means
        mean1 = calculate_mean(col1)
        mean2 = calculate_mean(col2)

        # Calculate covariance
        covariance = calculate_covariance(col1, col2, mean1, mean2)

        # Calculate standard deviations
        std_dev1 = calculate_std_dev(col1, mean1)
        std_dev2 = calculate_std_dev(col2, mean2)

        # Calculate correlation
        if std_dev1 != 0 and std_dev2 != 0:  # Avoid division by zero
            correlation = covariance / (std_dev1 * std_dev2)
        else:
            correlation = 0  # If any standard deviation is zero, correlation is zero

        # Store the correlation in the symmetric matrix
        corr_matrix[column_names[i]][column_names[j]] = correlation
        corr_matrix[column_names[j]][column_names[i]] = correlation  # Symmetry

# Print the correlation matrix
print("Correlation Matrix:")
for col1, row in corr_matrix.items():
    print(f"{col1}: {row}")
# print(len(corr_matrix))
# print(len(corr_matrix[0]))
data_frame = pd.DataFrame(corr_matrix) 
plt.figure(figsize=(10,8))
sns.heatmap(data_frame, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, center=0)
plt.title('Correlation Matrix Heatmap')
plt.show()


# Transpose the data to extract columns as lists
columns = list(zip(*dataList))

# Assign custom column names
column_names = ['X1', 'X2', 'X3', 'label']

# Function to calculate mean
def calculate_mean(column):
    return sum(column) / len(column)

# Function to calculate standard deviation
def calculate_std(column, mean):
    variance = sum((x - mean) ** 2 for x in column) / (len(column) - 1)
    return variance ** 0.5

# Function to calculate percentile
def calculate_percentile(column, percentile):
    sorted_col = sorted(column)
    index = (len(sorted_col) - 1) * percentile
    lower = int(index)
    upper = lower + 1
    weight = index - lower
    if upper < len(sorted_col):
        return sorted_col[lower] * (1 - weight) + sorted_col[upper] * weight
    else:
        return sorted_col[lower]

# Function to describe the data
def describe_data(columns, column_names):
    description = {}
    for i, col in enumerate(columns):
        col_data = list(col)
        mean = calculate_mean(col_data)
        std = calculate_std(col_data, mean)
        description[column_names[i]] = {
            'count': len(col_data),
            'mean': mean,
            'std': std,
            'min': min(col_data),
            '25%': calculate_percentile(col_data, 0.25),
            '50%': calculate_percentile(col_data, 0.50),
            '75%': calculate_percentile(col_data, 0.75),
            'max': max(col_data)
        }
     # Print the table header (including Max)
    header = "{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}".format(
        "Column", "Count", "Mean", "Std", "Min", "25%", "50%", "75%", "Max"
    )
    print(header)
    print("-" * len(header))

    # Print the rows with the stats (including Max)
    for col, stats in description.items():
        row = "{:<10} {:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}".format(
            col, stats['count'], stats['mean'], stats['std'], stats['min'],
            stats['25%'], stats['50%'], stats['75%'], stats['max']
        )
        print(row)

# Get description of the data
describe_data(columns, column_names)



import plotly.express as px

# print(dataList[0][3])
# print(len(dataList))
X1 = [dataList[i][0] for i in range(len(dataList))]
X2 = [dataList[i][1] for i in range(len(dataList))]
X3 = [dataList[i][2] for i in range(len(dataList))]
labels = [dataList[i][3] for i in range(len(dataList))]

fig = px.scatter_3d(
    x=X1,
    y=X2,
    z=X3,
    labels={'x': 'X1', 'y': 'X2', 'z': 'X3'},  # Axis labels
    title='3D Scatter Plot of the Moon Dataset'
)

fig.update_layout(
    width=1000,  # Set width (in pixels)
    height=800   # Set height (in pixels)
)
# Show the plot
fig.show()



labels_str = ['Class 0' if label == -1 else 'Class 1' for label in labels]

fig = px.scatter_3d(
    x=X1,
    y=X2,
    z=X3,
    color=labels_str,  # Color the points based on the label
    labels={'x': 'X1', 'y': 'X2', 'z': 'X3'},  # Axis labels
    title='3D Scatter Plot of the Moon Dataset'
)

fig.update_layout(
    width=1000,  # Set width (in pixels)
    height=800   # Set height (in pixels)
)
# Show the plot
fig.show()


# Function to count NA values
def count_na_values(dataList, column_names):
    # Initialize a dictionary to count NA values per column
    na_counts = {col: 0 for col in column_names}
    # Iterate through each row and column
    for row in dataList:
        for col_index, value in enumerate(row):
            # Check if the value is None (NA)
            if value is None:
                # Increment the corresponding column's NA count
                col_name = column_names[col_index]
                na_counts[col_name] += 1

    # Print the result
    print("Columns with NA values:")
    for column, count in na_counts.items():
        print(f"{column}: {count}")

# Call the function to count NA values
count_na_values(dataList, column_names)


def normalize_data(data):
    min_vals = [min(col) for col in zip(*data)]
    max_vals = [max(col) for col in zip(*data)]
    return [[(x - min_val) / (max_val - min_val) for x, min_val, max_val in zip(row, min_vals, max_vals)] for row in data]

# Call the function to normalize the dataList (excluding class column)
normalized_dataList = normalize_data_except_class(dataList, 3)

# Print normalized dataList
# for row in normalized_dataList:
#     print(row)

normalized_columns = list(zip(*normalized_dataList))

describe_data(columns, column_names)
print()
describe_data(normalized_columns, column_names)



X1 = [normalized_dataList[i][0] for i in range(len(normalized_dataList))]
X2 = [normalized_dataList[i][1] for i in range(len(normalized_dataList))]
X3 = [normalized_dataList[i][2] for i in range(len(normalized_dataList))]
labels = [normalized_dataList[i][3] for i in range(len(normalized_dataList))]

fig = px.scatter_3d(
    x=X1,
    y=X2,
    z=X3,
    color=labels_str,
    labels={'x': 'X1', 'y': 'X2', 'z': 'X3'},  # Axis labels
    title='3D Scatter Plot of the Moon Dataset'
)

fig.update_layout(
    width=1000,  # Set width (in pixels)
    height=800   # Set height (in pixels)
)
# Show the plot
fig.show()


import math
import random
import csv

# Helper functions to replace numpy operations
def mean(data):
    return sum(data) / len(data)

def std_dev(data):
    avg = mean(data)
    return math.sqrt(sum((x - avg) ** 2 for x in data) / len(data))

def dot_product(a, b):
    return sum(x * y for x, y in zip(a, b))

def matrix_multiply(A, B):
    return [[sum(a * b for a, b in zip(row, col)) for col in zip(*B)] for row in A]

def transpose(matrix):
    return list(map(list, zip(*matrix)))

def subtract_vectors(a, b):
    return [x - y for x, y in zip(a, b)]

def add_vectors(a, b):
    return [x + y for x, y in zip(a, b)]

def scalar_multiply(scalar, vector):
    return [scalar * x for x in vector]

# def element_wise_multiply(a, b):
#     return [x * y for x, y in zip(a, b)]



# Helper functions for matrix operations
def matrix_inverse(matrix):
    n = len(matrix)
    identity = [[1.0 if i == j else 0.0 for j in range(n)] for i in range(n)]
    augmented = [row + id_row for row, id_row in zip(matrix, identity)]

    for i in range(n):
        pivot = augmented[i][i]
        for j in range(i, 2*n):
            augmented[i][j] /= pivot
        for k in range(n):
            if k != i:
                factor = augmented[k][i]
                for j in range(i, 2*n):
                    augmented[k][j] -= factor * augmented[i][j]

    return [row[n:] for row in augmented]

def matrix_determinant(matrix):
    n = len(matrix)
    if n == 1:
        return matrix[0][0]
    if n == 2:
        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]
    det = 0
    for j in range(n):
        submatrix = [row[:j] + row[j+1:] for row in matrix[1:]]
        det += (-1)**j * matrix[0][j] * matrix_determinant(submatrix)
    return det


def normalize_data(data):
    min_vals = [min(col) for col in zip(*data)]
    max_vals = [max(col) for col in zip(*data)]
    return [[(x - min_val) / (max_val - min_val) for x, min_val, max_val in zip(row, min_vals, max_vals)] for row in data]

# Implement PCA
def pca(data, n_components):
    # Center the data
    mean_vector = [mean(col) for col in zip(*data)]
    centered_data = [subtract_vectors(row, mean_vector) for row in data]

    # Compute covariance matrix
    cov_matrix = [[sum(a * b for a, b in zip(col1, col2)) / (len(data) - 1)
                   for col2 in zip(*centered_data)] for col1 in zip(*centered_data)]

    # Compute eigenvectors and eigenvalues
    eigenvalues, eigenvectors = eigen_decomposition(cov_matrix)

    # Sort eigenvectors by eigenvalues in descending order
    sorted_indices = sorted(range(len(eigenvalues)), key=lambda k: eigenvalues[k], reverse=True)
    top_eigenvectors = [eigenvectors[i] for i in sorted_indices[:n_components]]

    # Project data onto principal components
    return [matrix_multiply([row], transpose(top_eigenvectors))[0] for row in centered_data]

# Simple implementation of eigendecomposition (power iteration method)
def eigen_decomposition(matrix, num_iterations=100):
    n = len(matrix)
    eigenvalues = []
    eigenvectors = []

    for _ in range(n):
        vector = [random.random() for _ in range(n)]
        for _ in range(num_iterations):
            new_vector = matrix_multiply(matrix, [[x] for x in vector])
            new_vector = [x[0] for x in new_vector]
            norm = math.sqrt(sum(x*x for x in new_vector))
            vector = [x / norm for x in new_vector]
        
        eigenvalue = dot_product(matrix_multiply(matrix, [[x] for x in vector])[0], vector)
        eigenvalues.append(eigenvalue)
        eigenvectors.append(vector)

        # Deflate the matrix
        outer_product = [[a * b for b in vector] for a in vector]
        matrix = [[matrix[i][j] - eigenvalue * outer_product[i][j] for j in range(n)] for i in range(n)]

    return eigenvalues, eigenvectors

# Gaussian Mixture Model implementation
class GaussianMixture:
    def __init__(self, n_components, n_features, max_iter=100, tol=1e-3):
        self.n_components = n_components
        self.n_features = n_features
        self.max_iter = max_iter
        self.tol = tol
    
    def initialize_parameters(self, X):
        n_samples = len(X)
        self.weights = [1.0 / self.n_components] * self.n_components
        self.means = random.sample(X, self.n_components)
        # Add a small value to the diagonal of the covariance matrices
        self.covariances = [[[1.0 if i == j else 0.0 for j in range(self.n_features)] for i in range(self.n_features)] for _ in range(self.n_components)]
        for cov in self.covariances:
            for i in range(self.n_features):
                cov[i][i] += 1e-6

    def gaussian_pdf(self, x, mean, cov):
        n = len(x)
        diff = subtract_vectors(x, mean)
        try:
            inv_cov = matrix_inverse(cov)
            det_cov = matrix_determinant(cov)
            if det_cov == 0:
                return 0.0
            exponent = -0.5 * dot_product(matrix_multiply([diff], inv_cov)[0], diff)
            return math.exp(exponent) / math.sqrt((2 * math.pi)**n * det_cov)
        except ZeroDivisionError:
            return 0.0
    
    def expectation_step(self, X):
        responsibilities = [[0.0 for _ in range(self.n_components)] for _ in range(len(X))]
        for i, x in enumerate(X):
            total = 0.0
            for k in range(self.n_components):
                responsibilities[i][k] = self.weights[k] * self.gaussian_pdf(x, self.means[k], self.covariances[k])
                total += responsibilities[i][k]
            responsibilities[i] = [r / total for r in responsibilities[i]]
        return responsibilities
    
    def maximization_step(self, X, responsibilities):
        n_samples = len(X)
        total_resp = [sum(r[k] for r in responsibilities) for k in range(self.n_components)]
        
        self.weights = [total / n_samples for total in total_resp]
        
        for k in range(self.n_components):
            self.means[k] = [sum(r[k] * x[j] for r, x in zip(responsibilities, X)) / total_resp[k] for j in range(self.n_features)]
            
            diff = [subtract_vectors(x, self.means[k]) for x in X]
            weighted_diff = [scalar_multiply(r[k], d) for r, d in zip(responsibilities, diff)]
            self.covariances[k] = [[sum(wd[i] * d[j] for wd, d in zip(weighted_diff, diff)) / total_resp[k] for j in range(self.n_features)] for i in range(self.n_features)]
    
    def fit(self, X):
        self.initialize_parameters(X)
        
        for _ in range(self.max_iter):
            prev_log_likelihood = self.log_likelihood(X)
            responsibilities = self.expectation_step(X)
            self.maximization_step(X, responsibilities)
            curr_log_likelihood = self.log_likelihood(X)
            
            if abs(curr_log_likelihood - prev_log_likelihood) < self.tol:
                break
    
    def log_likelihood(self, X):
        return sum(math.log(sum(self.weights[k] * self.gaussian_pdf(x, self.means[k], self.covariances[k]) for k in range(self.n_components))) for x in X)
    
    def predict(self, X):
        responsibilities = self.expectation_step(X)
        return [max(range(self.n_components), key=lambda k: r[k]) for r in responsibilities]



# Load and preprocess data
data = dataList
normalized_data = normalize_data(data)

# Separate features and labels
X = [row[:3] for row in normalized_data]
y = [int(row[3]) for row in normalized_data]

# Apply PCA
X_pca = pca(X, 2)

# Split data into training and testing sets
train_ratio = 0.8
split_index = int(len(X_pca) * train_ratio)
X_train, X_test = X_pca[:split_index], X_pca[split_index:]
y_train, y_test = y[:split_index], y[split_index:]

# Train GMMs for each class
gmm_class_0 = GaussianMixture(n_components=2, n_features=2)
gmm_class_1 = GaussianMixture(n_components=2, n_features=2)

X_train_0 = [x for x, label in zip(X_train, y_train) if label == 0]
X_train_1 = [x for x, label in zip(X_train, y_train) if label == 1]

gmm_class_0.fit(X_train_0)
gmm_class_1.fit(X_train_1)

# Predict on test set
def predict_gmm(X):
    log_likelihood_0 = [gmm_class_0.log_likelihood([x]) for x in X]
    log_likelihood_1 = [gmm_class_1.log_likelihood([x]) for x in X]
    return [int(l1 > l0) for l0, l1 in zip(log_likelihood_0, log_likelihood_1)]

y_pred = predict_gmm(X_test)

# Evaluate the model
accuracy = sum(1 for true, pred in zip(y_test, y_pred) if true == pred) / len(y_test)
print(f"Accuracy: {accuracy:.4f}")

# Compute confusion matrix
conf_matrix = [[0, 0], [0, 0]]
for true, pred in zip(y_test, y_pred):
    conf_matrix[true][pred] += 1
    
print("Confusion Matrix:")
print(conf_matrix)

# normalized_data


import math
import random
import csv

# Helper functions to replace numpy operations
def mean(data):
    return sum(data) / len(data)

def std_dev(data):
    avg = mean(data)
    return math.sqrt(sum((x - avg) ** 2 for x in data) / len(data))

def dot_product(a, b):
    return sum(x * y for x, y in zip(a, b))

def matrix_multiply(A, B):
    return [[sum(a * b for a, b in zip(row, col)) for col in zip(*B)] for row in A]

def transpose(matrix):
    return list(map(list, zip(*matrix)))

def subtract_vectors(a, b):
    return [x - y for x, y in zip(a, b)]

def add_vectors(a, b):
    return [x + y for x, y in zip(a, b)]

def scalar_multiply(scalar, vector):
    return [scalar * x for x in vector]

def element_wise_multiply(a, b):
    return [x * y for x, y in zip(a, b)]

# Helper functions for matrix operations
def matrix_inverse(matrix):
    n = len(matrix)
    identity = [[1.0 if i == j else 0.0 for j in range(n)] for i in range(n)]
    augmented = [row + id_row for row, id_row in zip(matrix, identity)]

    for i in range(n):
        pivot = augmented[i][i]
        for j in range(i, 2*n):
            augmented[i][j] /= pivot
        for k in range(n):
            if k != i:
                factor = augmented[k][i]
                for j in range(i, 2*n):
                    augmented[k][j] -= factor * augmented[i][j]

    return [row[n:] for row in augmented]

def matrix_determinant(matrix):
    n = len(matrix)
    if n == 1:
        return matrix[0][0]
    if n == 2:
        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]
    det = 0
    for j in range(n):
        submatrix = [row[:j] + row[j+1:] for row in matrix[1:]]
        det += (-1)**j * matrix[0][j] * matrix_determinant(submatrix)
    return det
    
def normalize_data(data):
    min_vals = [min(col) for col in zip(*data)]
    max_vals = [max(col) for col in zip(*data)]
    return [[(x - min_val) / (max_val - min_val) for x, min_val, max_val in zip(row, min_vals, max_vals)] for row in data]

# Implement PCA
def pca(data, n_components):
    # Center the data
    mean_vector = [mean(col) for col in zip(*data)]
    centered_data = [subtract_vectors(row, mean_vector) for row in data]

    # Compute covariance matrix
    cov_matrix = [[sum(a * b for a, b in zip(col1, col2)) / (len(data) - 1)
                   for col2 in zip(*centered_data)] for col1 in zip(*centered_data)]

    # Compute eigenvectors and eigenvalues
    eigenvalues, eigenvectors = eigen_decomposition(cov_matrix)

    # Sort eigenvectors by eigenvalues in descending order
    sorted_indices = sorted(range(len(eigenvalues)), key=lambda k: eigenvalues[k], reverse=True)
    top_eigenvectors = [eigenvectors[i] for i in sorted_indices[:n_components]]

    # Project data onto principal components
    return [matrix_multiply([row], transpose(top_eigenvectors))[0] for row in centered_data]

# Simple implementation of eigendecomposition (power iteration method)
def eigen_decomposition(matrix, num_iterations=100):
    n = len(matrix)
    eigenvalues = []
    eigenvectors = []

    for _ in range(n):
        vector = [random.random() for _ in range(n)]
        for _ in range(num_iterations):
            new_vector = matrix_multiply(matrix, [[x] for x in vector])
            new_vector = [x[0] for x in new_vector]
            norm = math.sqrt(sum(x*x for x in new_vector))
            vector = [x / norm for x in new_vector]
        
        eigenvalue = dot_product(matrix_multiply(matrix, [[x] for x in vector])[0], vector)
        eigenvalues.append(eigenvalue)
        eigenvectors.append(vector)

        # Deflate the matrix
        outer_product = [[a * b for b in vector] for a in vector]
        matrix = [[matrix[i][j] - eigenvalue * outer_product[i][j] for j in range(n)] for i in range(n)]

    return eigenvalues, eigenvectors

# Gaussian Mixture Model implementation
class GaussianMixture:
    def __init__(self, n_components, n_features, max_iter=100, tol=1e-3):
        self.n_components = n_components
        self.n_features = n_features
        self.max_iter = max_iter
        self.tol = tol
    
    def initialize_parameters(self, X):
        n_samples = len(X)
        self.weights = [1.0 / self.n_components] * self.n_components
        self.means = random.sample(X, self.n_components)
        # Add a small value to the diagonal of the covariance matrices
        self.covariances = [[[1.0 if i == j else 0.0 for j in range(self.n_features)] for i in range(self.n_features)] for _ in range(self.n_components)]
        for cov in self.covariances:
            for i in range(self.n_features):
                cov[i][i] += 1e-6

    def gaussian_pdf(self, x, mean, cov):
        n = len(x)
        diff = subtract_vectors(x, mean)
        try:
            inv_cov = matrix_inverse(cov)
            det_cov = matrix_determinant(cov)
            if det_cov == 0:
                return 0.0
            exponent = -0.5 * dot_product(matrix_multiply([diff], inv_cov)[0], diff)
            return math.exp(exponent) / math.sqrt((2 * math.pi)**n * det_cov)
        except ZeroDivisionError:
            return 0.0
    
    def expectation_step(self, X):
        responsibilities = [[0.0 for _ in range(self.n_components)] for _ in range(len(X))]
        for i, x in enumerate(X):
            total = 0.0
            for k in range(self.n_components):
                responsibilities[i][k] = self.weights[k] * self.gaussian_pdf(x, self.means[k], self.covariances[k])
                total += responsibilities[i][k]
            responsibilities[i] = [r / total for r in responsibilities[i]]
        return responsibilities
    
    def maximization_step(self, X, responsibilities):
        n_samples = len(X)
        total_resp = [sum(r[k] for r in responsibilities) for k in range(self.n_components)]
        
        self.weights = [total / n_samples for total in total_resp]
        
        for k in range(self.n_components):
            self.means[k] = [sum(r[k] * x[j] for r, x in zip(responsibilities, X)) / total_resp[k] for j in range(self.n_features)]
            
            diff = [subtract_vectors(x, self.means[k]) for x in X]
            weighted_diff = [scalar_multiply(r[k], d) for r, d in zip(responsibilities, diff)]
            self.covariances[k] = [[sum(wd[i] * d[j] for wd, d in zip(weighted_diff, diff)) / total_resp[k] for j in range(self.n_features)] for i in range(self.n_features)]
    
    def fit(self, X):
        self.initialize_parameters(X)
        
        for _ in range(self.max_iter):
            prev_log_likelihood = self.log_likelihood(X)
            responsibilities = self.expectation_step(X)
            self.maximization_step(X, responsibilities)
            curr_log_likelihood = self.log_likelihood(X)
            
            if abs(curr_log_likelihood - prev_log_likelihood) < self.tol:
                break
    
    def log_likelihood(self, X):
        return sum(math.log(sum(self.weights[k] * self.gaussian_pdf(x, self.means[k], self.covariances[k]) for k in range(self.n_components))) for x in X)
    
    def predict(self, X):
        responsibilities = self.expectation_step(X)
        return [max(range(self.n_components), key=lambda k: r[k]) for r in responsibilities]







