 import pandas as pd
import numpy as np

# Load the dataset
file_path = 'datasets/moonDataset.csv'
moon_dataset = pd.read_csv(file_path)

# Show the first few rows of the dataset to understand its structure
moon_dataset.head()


X = moon_dataset[['X1', 'X2', 'X3']].values
y_true = moon_dataset['label'].values  # True labels for evaluation
# n_samples, n_features = X.shape
n_samples = len(X)
n_features = len(X[0])


# Use the standard inline backend
%matplotlib inline

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import animation

# Extracting the features and labels
X1 = moon_dataset['X1']
X2 = moon_dataset['X2']
X3 = moon_dataset['X3']
labels = moon_dataset['label']

# Plotting the data in 3D with animation
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot, using different colors for different classes
scatter_0 = ax.scatter(X1[labels == 0], X2[labels == 0], X3[labels == 0], c='blue', label='Class 0', s=50)
scatter_1 = ax.scatter(X1[labels == 1], X2[labels == 1], X3[labels == 1], c='red', label='Class 1', s=50)

# Labels and title
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('X3')
ax.set_title('3D Scatter Plot of the Moon Dataset')
ax.legend()

# Function to rotate the plot
def rotate(angle):
    ax.view_init(azim=angle)

# Create animation
ani = animation.FuncAnimation(fig, rotate, frames=range(0, 360, 2), interval=100)
plt.show()



     


# Number of Gaussians (clusters) we want to fit
n_components = 2

# Initialize means (randomly select two data points as initial means)
np.random.seed(42)
means = X[np.random.choice(n_samples, n_components, replace=False)]

# Initialize covariances (identity matrices for each component)
covariances = np.array([np.eye(n_features) for _ in range(n_components)])

# Initialize the mixing coefficients (priors)
priors = np.ones(n_components) / n_components


def multivariate_gaussian(X, mean, cov):
    """Calculate the multivariate Gaussian probability."""
    d = X.shape[1]
    cov_inv = np.linalg.inv(cov)
    diff = X - mean
    exponent = np.exp(-0.5 * np.sum(np.dot(diff, cov_inv) * diff, axis=1))
    return (1.0 / np.sqrt((2 * np.pi) ** d * np.linalg.det(cov))) * exponent


def e_step(X, means, covariances, priors):
    """E-step: Calculate responsibilities based on current parameters."""
    n_samples = X.shape[0]
    responsibilities = np.zeros((n_samples, n_components))
    
    # Calculate the responsibility (posterior probability) for each data point
    for k in range(n_components):
        responsibilities[:, k] = priors[k] * multivariate_gaussian(X, means[k], covariances[k])
    
    # Normalize responsibilities
    responsibilities /= responsibilities.sum(axis=1, keepdims=True)
    
    return responsibilities


def m_step(X, responsibilities):
    """M-step: Update the parameters based on responsibilities."""
    N_k = responsibilities.sum(axis=0)  # Total "weight" assigned to each component
    means = np.dot(responsibilities.T, X) / N_k[:, None]
    
    covariances = np.zeros((n_components, n_features, n_features))
    for k in range(n_components):
        diff = X - means[k]
        covariances[k] = np.dot(responsibilities[:, k] * diff.T, diff) / N_k[k]
    
    priors = N_k / n_samples
    return means, covariances, priors


# Parameters
max_iters = 10000
tol = 1e-4

# EM Algorithm loop
for iteration in range(max_iters):
    # E-step
    responsibilities = e_step(X, means, covariances, priors)
    
    # M-step
    new_means, new_covariances, new_priors = m_step(X, responsibilities)
    
    # Check for convergence (based on means)
    if np.linalg.norm(new_means - means) < tol:
        print(f'Converged at iteration {iteration}')
        break
    
    # Update parameters
    means, covariances, priors = new_means, new_covariances, new_priors


# Predict the class for each data point
predictions = responsibilities.argmax(axis=1)

# Evaluate accuracy
accuracy = np.mean(predictions == y_true)
print(f'Accuracy: {accuracy * 100:.2f}%')


X = moon_dataset[['X1', 'X2', 'X3']].values
y = moon_dataset['label'].values

# Perform SVD
U, s, Vt = np.linalg.svd(X.T) # We transpose the data because we want to perform SVD on the features, not the samples.
print(f"U: {U}")
print(f"s: {s}")
print(f"V.T: {Vt}")


# Choose the first two singular values
k = 2
X_hat = (U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :])
# Reduce dimension
X_reduced = (U[:, :k].T @ X_hat).T

X_reduced


# Extracting x and y coordinates
x = X_reduced[:, 0]
y = X_reduced[:, 1]

# Creating a scatter plot
plt.scatter(x, y, color='blue', marker='o')

# Adding labels and title
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Scatter Plot')

# Displaying the plot
plt.show()


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Load the data
data = pd.read_csv('datasets/moonDataset.csv')
df_normalized = (data - data.min()) / (data.max() - data.min())

data= df_normalized

X = data[['X1', 'X2', 'X3']].values
y = data['label'].values
# Perform PCA
pca = PCA(n_components=2)
data_pca = pca.fit_transform(X,y)

# Extracting x and y coordinates
x = data_pca[:, 0]
y = data_pca[:, 1]

# Creating a scatter plot
plt.scatter(x, y, color='blue', marker='o')

# Adding labels and title
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Scatter Plot')

# Displaying the plot
plt.show()


import numpy as np
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
# from sklearn.datasets import load_iris

# Load dataset (example: Iris dataset)
data = pd.read_csv('datasets/moonDataset.csv')
X = data[['X1', 'X2', 'X3']].values
y = data['label'].values

# Apply LDA to reduce the data to 2 components, considering the target labels
lda = LDA(n_components=2)
X_lda = lda.fit_transform(X, y)

# Plot the LDA result
plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis', marker='o')
plt.title("LDA on Iris Dataset")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.show()


labeled_data = []
labels = moon_dataset['label']
for i in range(len(X_reduced)):
    labeled_data.append(X_reduced[i] + [labels[i]])
labeled_data


labeled_array = np.column_stack((X_reduced, labels))


labeled_array


X = labeled_array[:, :2]  # features (first two columns)
y_true = labeled_array[:, 2]  # true class labels (last column)

# Helper function for multivariate Gaussian PDF
def gaussian_pdf(X, mean, cov):
    n = X.shape[1]  # Number of dimensions
    diff = X - mean
    return (1.0 / (np.sqrt((2 * np.pi)**n * np.linalg.det(cov)))) * \
           np.exp(-0.5 * np.sum(np.dot(diff, np.linalg.inv(cov)) * diff, axis=1))

# Initialize parameters (means, covariances, and weights for two classes)
np.random.seed(42)
k = 2  # Number of classes (0 and 1)
n, d = X.shape  # Number of data points and dimension

# Randomly initialize means
means = np.array([X[np.random.choice(n)] for _ in range(k)])
# Initialize covariances as identity matrices
covariances = np.array([np.eye(d) for _ in range(k)])
# Initialize weights as uniform
weights = np.array([1/k for _ in range(k)])

# EM algorithm parameters
max_iter = 100
tol = 1e-4  # Convergence threshold

# EM algorithm
for iteration in range(max_iter):
    # E-step: calculate the responsibilities (probabilities of each point belonging to a class)
    responsibilities = np.zeros((n, k))
    
    for i in range(k):
        responsibilities[:, i] = weights[i] * gaussian_pdf(X, means[i], covariances[i])
    
    # Normalize to make probabilities sum to 1 for each point
    responsibilities /= responsibilities.sum(axis=1, keepdims=True)
    
    # M-step: update weights, means, and covariances
    Nk = responsibilities.sum(axis=0)  # Effective number of points assigned to each class
    
    # Update weights
    weights = Nk / n
    
    # Update means
    means = np.dot(responsibilities.T, X) / Nk[:, np.newaxis]
    
    # Update covariances
    for i in range(k):
        diff = X - means[i]
        covariances[i] = np.dot((responsibilities[:, i][:, np.newaxis] * diff).T, diff) / Nk[i]
    
    # Check for convergence (change in means)
    if np.all(np.abs(means - means) < tol):
        break

# Classify each point based on the highest responsibility
y_pred = np.argmax(responsibilities, axis=1)

# Compare predicted classes to true labels
accuracy = np.mean(y_pred == y_true)
print("Accuracy:", accuracy)


X = moon_dataset[['X1', 'X2', 'X3']].values # features (first two columns)
y_true = moon_dataset['label'].values  # true class labels (last column)

# Helper function for multivariate Gaussian PDF
def gaussian_pdf(X, mean, cov):
    n = X.shape[1]  # Number of dimensions
    diff = X - mean
    return (1.0 / (np.sqrt((2 * np.pi)**n * np.linalg.det(cov)))) * \
           np.exp(-0.5 * np.sum(np.dot(diff, np.linalg.inv(cov)) * diff, axis=1))

# Initialize parameters (means, covariances, and weights for two classes)
np.random.seed(42)
k = 2  # Number of classes (0 and 1)
n, d = X.shape  # Number of data points and dimension

# Randomly initialize means
means = np.array([X[np.random.choice(n)] for _ in range(k)])
# Initialize covariances as identity matrices
covariances = np.array([np.eye(d) for _ in range(k)])
# Initialize weights as uniform
weights = np.array([1/k for _ in range(k)])

# EM algorithm parameters
max_iter = 100
tol = 1e-4  # Convergence threshold

# EM algorithm
for iteration in range(max_iter):
    # E-step: calculate the responsibilities (probabilities of each point belonging to a class)
    responsibilities = np.zeros((n, k))
    
    for i in range(k):
        responsibilities[:, i] = weights[i] * gaussian_pdf(X, means[i], covariances[i])
    
    # Normalize to make probabilities sum to 1 for each point
    responsibilities /= responsibilities.sum(axis=1, keepdims=True)
    
    # M-step: update weights, means, and covariances
    Nk = responsibilities.sum(axis=0)  # Effective number of points assigned to each class
    
    # Update weights
    weights = Nk / n
    
    # Update means
    means = np.dot(responsibilities.T, X) / Nk[:, np.newaxis]
    
    # Update covariances
    for i in range(k):
        diff = X - means[i]
        covariances[i] = np.dot((responsibilities[:, i][:, np.newaxis] * diff).T, diff) / Nk[i]
    
    # Check for convergence (change in means)
    if np.all(np.abs(means - means) < tol):
        break

# Classify each point based on the highest responsibility
y_pred = np.argmax(responsibilities, axis=1)

# Compare predicted classes to true labels
accuracy = np.mean(y_pred == y_true)
print("Accuracy:", accuracy)


import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture

# Assuming X is your dataset with the first two columns as features
gmm = GaussianMixture(n_components=2, covariance_type='full')
gmm.fit(X_reduced)  # Fit the GMM model on your dataset
gmm_labels = gmm.predict(X)  # Predict the labels

# Plot the clusters
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=gmm_labels, cmap='viridis', marker='o', edgecolor='k', s=80)

# Adding labels and title
plt.title('GMM Clustering Visualization')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

# Show the plot
plt.show()



